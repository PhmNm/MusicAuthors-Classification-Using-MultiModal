{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wLBwwvbwsYGQ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AUlSbvSz64n9"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaYR_LHk2DpK"},"outputs":[],"source":["import re\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from gensim.utils import simple_preprocess\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoTokenizer, AutoModel, logging\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","logging.set_verbosity_error()"]},{"cell_type":"markdown","metadata":{"id":"90e_pmfu0s-Y"},"source":["## LOAD DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALXsEoi5NgEL"},"outputs":[],"source":["sub_path = '/content/drive/MyDrive/khoa_luan/data_04/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1O80NPh1aydM"},"outputs":[],"source":["# X_train.to_csv(sub_path + '/train.csv', index=False)\n","train_df = pd.read_csv(sub_path + '/train.csv')\n","train_emb = np.load(sub_path + '/train_embedding_1024_vi.npy', allow_pickle=True)\n","train_df['audio_embedding'] = train_emb\n","\n","train_df['author'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3-qigXga2ic"},"outputs":[],"source":["# X_test.to_csv(sub_path + '/test.csv', index=False)\n","test_df = pd.read_csv(sub_path + '/test.csv')\n","test_emb = np.load(sub_path + '/test_embedding_1024_vi.npy', allow_pickle=True)\n","test_df['audio_embedding'] = test_emb\n","print()\n","test_df['author'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVWsTodfRdzm"},"outputs":[],"source":["# X_val.to_csv(sub_path + '/val.csv', index=False)\n","val_df = pd.read_csv(sub_path + '/val.csv')\n","val_emb = np.load(sub_path + '/val_embedding_1024_vi.npy', allow_pickle=True)\n","val_df['audio_embedding'] = val_emb\n","\n","val_df['author'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"0q35l-lyRtvN"},"source":["## MODELING"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fAI1sFgR-x-"},"outputs":[],"source":["phobert_type = 'vinai/phobert-base-v2'\n","# audio_model_type = 'facebook/wav2vec2-large-960h-lv60-self'\n","audio_model_type = 'nguyenvulebinh/wav2vec2-base-vietnamese-250h'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lbHDipM95uw7"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(phobert_type, use_fast=False)"]},{"cell_type":"markdown","metadata":{"id":"gxFpZpW3-dQ3"},"source":["### NN MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqxhsuEg-cyR"},"outputs":[],"source":["class Multimodal_Dataset(Dataset):\n","    def __init__(self, df, tokenizer, max_len, label_encoder, device):\n","      self.df = df\n","      self.max_len = max_len\n","      self.tokenizer = tokenizer\n","      self.label_encoder = label_encoder\n","      self.device = device\n","    def __len__(self):\n","      return len(self.df)\n","\n","    def __getitem__(self, index):\n","      row = self.df.iloc[index]\n","      text, audio, label = self.get_input_data(row)\n","      audio = torch.tensor(audio).float()\n","      encoding = self.tokenizer.__call__(\n","          text,\n","          truncation=True,\n","          add_special_tokens=True,\n","          max_length=self.max_len,\n","          padding='max_length',\n","          return_attention_mask=True,\n","          return_token_type_ids=False,\n","          return_tensors='pt',\n","      )\n","\n","      return {\n","          'audio_input_values': audio[0],\n","          'audio_attention_mask': audio[1],\n","          'input_ids': encoding['input_ids'].flatten(),\n","          'attention_mask': encoding['attention_mask'].flatten(),\n","          'target': torch.tensor(label, dtype=torch.long),\n","      }\n","\n","\n","    def get_input_data(self, row):\n","      text = row['lyric']\n","      audio = row['audio_embedding']\n","      label = self.label_encoder[row['author']]\n","\n","      return text, audio, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IVOwGUj0Tqy"},"outputs":[],"source":["class Multimodal_Classifier(nn.Module):\n","    def __init__(self, n_classes):\n","        super(Multimodal_Classifier, self).__init__()\n","\n","        self.bert = AutoModel.from_pretrained(phobert_type)\n","        bert_out_size = self.bert.config.hidden_size\n","\n","        self.w2v2 = AutoModel.from_pretrained(audio_model_type)\n","        self.w2v2.config.mask_time_prob = 0\n","        w2v2_out_size = self.w2v2.config.output_hidden_size\n","\n","        embed_dim = 256\n","        num_heads = 8\n","        self.multihead_att = nn.MultiheadAttention(\n","            embed_dim=embed_dim,\n","            num_heads=num_heads,\n","            batch_first=True\n","        )\n","\n","        self.conv1d_text = nn.Conv1d(\n","              in_channels=256,\n","              out_channels=256,\n","              kernel_size=3,\n","              stride=3,\n","              padding=1\n","        )\n","\n","        self.conv1d_audio = nn.Sequential(\n","            nn.Conv1d(\n","                in_channels=2,\n","                out_channels=2,\n","                kernel_size=3,\n","                stride=3,\n","                padding=1\n","            ),\n","            nn.Linear(256,256)\n","        )\n","\n","        self.bi_lstm = nn.LSTM(\n","            input_size = 256,\n","            hidden_size = 128,\n","            batch_first = True,\n","            bidirectional = True,\n","        )\n","        self.dropout = nn.Dropout(p=0.2)\n","        self.last_fc = nn.Linear(512, n_classes)\n","\n","    def merged_strategy(self,hidden_states,mode=None):\n","        if mode == 'mean':\n","            outputs = torch.mean(hidden_states, dim=1)\n","        elif mode == 'sum':\n","            outputs = torch.sum(hidden_states, dim=1)\n","        elif mode == 'max':\n","            outputs = torch.max(hidden_states, dim=1)[0]\n","        else:\n","            outputs = hidden_states[:,0]\n","        return outputs\n","\n","    def forward(self, input):\n","        ## TEXT LAYERS\n","        last_hidden_state, output_bert = self.bert(\n","            input_ids=input['input_ids'],\n","            attention_mask=input['attention_mask'],\n","            return_dict=False\n","        )\n","        x_text = self.conv1d_text(last_hidden_state)\n","\n","        ## AUDIO LAYERS\n","        output_w2v2 = self.w2v2(\n","            input_values=input['audio_input_values'],\n","            attention_mask=input['audio_attention_mask'],\n","          )\n","\n","\n","        x_audio = self.conv1d_audio(output_w2v2.last_hidden_state)\n","        x_audio, (h, c) = self.bi_lstm(x_audio)\n","        x_audio = self.dropout(x_audio)\n","\n","        ## CO-ATTENTION\n","        H_t, H_t_weights = self.multihead_att(\n","            query=x_audio,\n","            key=x_text,\n","            value=x_text\n","        )\n","\n","        H_a, H_a_weights = self.multihead_att(\n","            query=x_text,\n","            key=x_audio,\n","            value=x_audio\n","        )\n","\n","        H_t = self.merged_strategy(H_t, mode='mean')\n","        H_a = self.merged_strategy(H_a, mode='mean')\n","\n","        ## MUL/CAT\n","\n","        x_cat = torch.cat((H_a, H_t),1)\n","\n","        ## LAST LINEARS\n","        x = self.last_fc(x_cat)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IUalYmMP0boX"},"outputs":[],"source":["def train_single_epoch(model, train_loader, val_loader, loss_func, optimizer, device):\n","    model.train()\n","    train_losses = []\n","    val_losses = []\n","    train_acc = 0\n","    val_acc = 0\n","\n","    for data in train_loader:\n","        input = {\n","            'input_ids':data['input_ids'].to(device),\n","            'attention_mask':data['attention_mask'].to(device),\n","            'audio_input_values':data['audio_input_values'].to(device),\n","            'audio_attention_mask':data['audio_attention_mask'].to(device)\n","        }\n","        targets = data['target'].to(device)\n","\n","        # calculate loss\n","        outputs = model(input)\n","        loss = loss_func(outputs, targets)\n","        _, predictions = torch.max(outputs, dim=1)\n","        train_acc += torch.sum(predictions == targets)\n","\n","        # backpropagate error and update weights\n","        optimizer.zero_grad()\n","        train_losses.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    for data in val_loader:\n","        input = {\n","            'input_ids':data['input_ids'].to(device),\n","            'attention_mask':data['attention_mask'].to(device),\n","            'audio_input_values':data['audio_input_values'].to(device),\n","            'audio_attention_mask':data['audio_attention_mask'].to(device)\n","        }\n","        targets = data['target'].to(device)\n","\n","        # calculate loss\n","        outputs = model(input)\n","\n","        loss = loss_func(outputs, targets)\n","        _, predictions = torch.max(outputs, dim=1)\n","        val_acc += torch.sum(predictions == targets)\n","        val_losses.append(loss.item())\n","\n","    train_acc = train_acc.double()/len(train_loader.dataset)\n","    val_acc = val_acc.double()/len(val_loader.dataset)\n","    train_loss = np.mean(train_losses)\n","    val_loss = np.mean(val_losses)\n","\n","    print(f'Train Accuracy: {train_acc}')\n","    print(f'Validation Accuracy: {val_acc}')\n","    print(f'Train Loss: {train_loss}')\n","    print(f'Validate Loss: {val_loss}')\n","    return train_loss, val_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5M8BdHLdgxv8"},"outputs":[],"source":["def train(model, train_loader, val_loader, loss_func, optimizer, epochs, device):\n","  train_losses = []\n","  val_losses = []\n","  for i in range(epochs):\n","      print(f\"Epoch {i+1}\")\n","      train_loss, val_loss = train_single_epoch(model, train_loader, val_loader, loss_func, optimizer, device)\n","      train_losses.append(train_loss)\n","      val_losses.append(val_loss)\n","      print(\"---------------------------\")\n","  print(\"Finished training\")\n","  return train_losses, val_losses"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbXw3Zn2_SMT"},"outputs":[],"source":["def prepare_loaders(train_df, val_df, test_df, max_len, batch_size):\n","\n","    train_dataset = Multimodal_Dataset(\n","        df=train_df,\n","        tokenizer=tokenizer,\n","        max_len=max_len,\n","        label_encoder=label_encoder,\n","        device=device)\n","    val_dataset = Multimodal_Dataset(\n","        df=val_df,\n","        tokenizer=tokenizer,\n","        max_len=max_len,\n","        label_encoder=label_encoder,\n","        device=device)\n","    test_dataset = Multimodal_Dataset(\n","        df=test_df,\n","        tokenizer=tokenizer,\n","        max_len=max_len,\n","        label_encoder=label_encoder,\n","        device=device)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","    return train_loader, test_loader, val_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6eRpoNUW9Hf"},"outputs":[],"source":["label_encoder = {\n","    'khắc hưng': 0,\n","    'châu đăng khoa': 1,\n","    'khắc việt': 2,\n","    'phúc trường': 3,\n","    'nguyễn đình vũ': 4,\n","    'mr siro': 5,\n","    'vương anh tú': 6,\n","    'trịnh công sơn': 7,\n","    'phan mạnh quỳnh': 8,\n","    'nguyên chấn phong': 9,\n","    'nguyễn hồng thuận':10,\n","    'nguyễn văn chung': 11,\n","    'phạm trưởng': 12,\n","    'khánh đơn': 13,\n","    'tiên cookie': 14,\n","}\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uT09Eg5-EHXh"},"outputs":[],"source":["MAX_LEN = 256\n","BATCH_SIZE = 24\n","n_classes = len(label_encoder.keys())\n","\n","train_loader, test_loader, val_loader = prepare_loaders(train_df, val_df, test_df, MAX_LEN, BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRBhPJGXsI1l"},"outputs":[],"source":["model = Multimodal_Classifier(n_classes=n_classes).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ii8YG98A79Q"},"outputs":[],"source":["### freeze all layers of Pretrained PhoBert\n","# for param in model.bert.parameters():\n","#   param.requires_grad = False\n","\n","### freeze n layers of Pretrained Wav2vec2\n","for param in model.w2v2.feature_extractor.parameters():\n","  param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75JJFWS-MZjX"},"outputs":[],"source":["EPOCHS = 30\n","LR = 1e-5\n","loss_func = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=LR)\n","\n","train_loss, val_loss = train(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    loss_func=loss_func,\n","    optimizer=optimizer,\n","    epochs=EPOCHS,\n","    device=device\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7OniJw8EJBK"},"outputs":[],"source":["epochs = range(1, EPOCHS + 1)\n","# Plot and label the training and validation loss values\n","plt.plot(epochs, train_loss, label='Training Loss')\n","plt.plot(epochs, val_loss, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","# Set the tick locations\n","plt.xticks(range(0, EPOCHS + 1, EPOCHS//10))\n","\n","# Display the plot\n","plt.legend(loc='best')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4AEmP3t0j1X"},"outputs":[],"source":["model_path = sub_path + 'cm_bert_wav2vec2_model_checkpoints.pth'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dnfwtPxGAM6"},"outputs":[],"source":["checkpoint = {\n","    'epoch': EPOCHS + 1,\n","    'state_dict': model.state_dict(),\n","    'optimizer': optimizer.state_dict()\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Z7oImhP5dvN"},"outputs":[],"source":["torch.save(checkpoint, model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhIYbzJTHI--"},"outputs":[],"source":["def load_ckp(checkpoint_fpath, model, optimizer, device):\n","    checkpoint = torch.load(checkpoint_fpath, map_location=torch.device(device))\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['epoch']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWvJqe8a-lGW"},"outputs":[],"source":["# model, optimizer, epoch = load_ckp(model_path, model, optimizer, device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uYFbVdKgVKzi"},"outputs":[],"source":["def test(model, data_loader):\n","  model.eval()\n","  predicts = []\n","  predict_probs = []\n","  real_values = []\n","  for data in data_loader:\n","      input_ids = data['input_ids'].to(device)\n","      attention_mask = data['attention_mask'].to(device)\n","      audio_input_values = data['audio_input_values'].to(device)\n","      audio_attention_mask = data['audio_attention_mask'].to(device)\n","\n","      input = {\n","          'input_ids':input_ids,\n","          'attention_mask':attention_mask,\n","          'audio_input_values':audio_input_values,\n","          'audio_attention_mask':audio_attention_mask\n","      }\n","      targets = data['target'].to(device)\n","\n","      total_outs = []\n","      with torch.no_grad():\n","        outputs = model(input)\n","        total_outs.append(outputs)\n","\n","      total_outs = torch.stack(total_outs)\n","      _, pred = torch.max(total_outs.mean(0), dim=1)\n","      predicts.extend(pred)\n","      real_values.extend(targets)\n","\n","  predicts = torch.stack(predicts).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return real_values, predicts\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqr6c6onZULJ"},"outputs":[],"source":["real_values, pred_values = test(model, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKi38ER7eTRD"},"outputs":[],"source":["print('Precision Score: ', round(precision_score(real_values, pred_values, average='macro'),6))\n","print('Recall Score: ', round(recall_score(real_values, pred_values, average='macro'),6))\n","print('F1 Score: ', round(f1_score(real_values, pred_values, average='macro'),6))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ajgVQMCM5yU"},"outputs":[],"source":["target_names = label_encoder.keys()\n","print(classification_report(real_values, pred_values, target_names=target_names))"]},{"cell_type":"code","source":["labels = label_encoder.keys()\n","\n","\n","disp = ConfusionMatrixDisplay.from_predictions(real_values, pred_values, display_labels=labels, cmap='Oranges')\n","\n","\n","plt.xticks(fontsize=8)\n","\n","plt.gcf().autofmt_xdate()\n","plt.show()\n","\n","fig_name = 'cma_model_cm.png'\n","disp.figure_.savefig(sub_path + fig_name, format='png', bbox_inches='tight')"],"metadata":{"id":"2TCg0MQVKSLi"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}