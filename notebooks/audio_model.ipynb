{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"McG3Rj9qlvvI"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OR-twLwIMEan"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","source":["from transformers import AutoModel, logging\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch.optim import AdamW\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report, ConfusionMatrixDisplay\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","logging.set_verbosity_error()"],"metadata":{"id":"WY-Jf2nMS01o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DATA LOADING"],"metadata":{"id":"3VGvTO8s86p0"}},{"cell_type":"code","source":["sub_path = 'drive/MyDrive/khoa_luan/data_04/'"],"metadata":{"id":"traMdRCm26qw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1O80NPh1aydM"},"outputs":[],"source":["train_df = pd.read_csv(sub_path + '/train.csv')\n","train_emb = np.load(sub_path + '/train_embedding_1024_vi.npy', allow_pickle=True)\n","train_df['audio_embedding'] = train_emb\n","\n","train_df['author'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3-qigXga2ic"},"outputs":[],"source":["test_df = pd.read_csv(sub_path + '/test.csv')\n","test_emb = np.load(sub_path + '/test_embedding_1024_vi.npy', allow_pickle=True)\n","test_df['audio_embedding'] = test_emb\n","print()\n","test_df['author'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nVWsTodfRdzm"},"outputs":[],"source":["val_df = pd.read_csv(sub_path + '/val.csv')\n","val_emb = np.load(sub_path + '/val_embedding_1024_vi.npy', allow_pickle=True)\n","val_df['audio_embedding'] = val_emb\n","\n","val_df['author'].value_counts()"]},{"cell_type":"markdown","source":["## MODELING"],"metadata":{"id":"0hMGmpl28WWT"}},{"cell_type":"code","source":["audio_model_type = 'nguyenvulebinh/wav2vec2-base-vietnamese-250h'"],"metadata":{"id":"eWX5p2ocKyD1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Audio_Dataset(Dataset):\n","    def __init__(self, df, label_encoder, device):\n","      self.df = df\n","      self.label_encoder = label_encoder\n","      self.device = device\n","    def __len__(self):\n","      return len(self.df)\n","\n","    def __getitem__(self, index):\n","      row = self.df.iloc[index]\n","      audio, label = self.get_input_data(row)\n","      audio = torch.tensor(audio).float()\n","\n","      return {\n","          'input_values': audio[0],\n","          'attention_mask': audio[1],\n","          'target': torch.tensor(label, dtype=torch.long),\n","      }\n","\n","\n","    def get_input_data(self, row):\n","      audio = row['audio_embedding']\n","      label = self.label_encoder[row['author']]\n","\n","      return audio, label"],"metadata":{"id":"PgI8inUnmHCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Audio_Classifier(nn.Module):\n","    def __init__(self, n_classes):\n","        super(Audio_Classifier, self).__init__()\n","        self.w2v2 = AutoModel.from_pretrained(audio_model_type)\n","\n","        # self.w2v2.config.apply_spec_augment = False\n","        self.w2v2.config.mask_time_prob = 0\n","        w2v2_hidden_size = self.w2v2.config.output_hidden_size\n","\n","        self.dense = nn.Sequential(\n","            nn.Linear(w2v2_hidden_size, 512),\n","            nn.Linear(512, 512),\n","            nn.Dropout(p=0.1)\n","        )\n","        self.tanh = nn.Tanh()\n","        self.last_fc = nn.Linear(512, n_classes)\n","\n","    def merged_strategy(self,hidden_states,mode=None):\n","        if mode == 'mean':\n","            outputs = torch.mean(hidden_states, dim=1)\n","        elif mode == 'sum':\n","            outputs = torch.sum(hidden_states, dim=1)\n","        elif mode == 'max':\n","            outputs = torch.max(hidden_states, dim=1)[0]\n","        else:\n","            outputs = hidden_states[:,0]\n","        return outputs\n","\n","    def forward(self, input_data):\n","        output_w2v2 = self.w2v2(\n","            input_values=input_data['input_values'],\n","            attention_mask=input_data['attention_mask']\n","        )\n","        pooler = self.merged_strategy(output_w2v2.last_hidden_state, mode='mean')\n","        x = self.tanh(pooler)\n","        x = self.dense(x)\n","        x = self.tanh(x)\n","        x = self.last_fc(x)\n","\n","        return x"],"metadata":{"id":"mPFecnSIjK1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_single_epoch(model, train_loader, val_loader, loss_func, optimizer, device):\n","    model.train()\n","    train_losses = []\n","    val_losses = []\n","    train_acc = 0\n","    val_acc = 0\n","    for data in train_loader:\n","\n","        input = {\n","            'input_values': data['input_values'].to(device),\n","            'attention_mask': data['attention_mask'].to(device)\n","        }\n","        targets = data['target'].to(device)\n","\n","        # calculate loss\n","        outputs = model(input)\n","\n","        loss = loss_func(outputs, targets)\n","        _, predictions = torch.max(outputs, dim=1)\n","        train_acc += torch.sum(predictions == targets)\n","\n","        # backpropagate error and update weights\n","        optimizer.zero_grad()\n","        train_losses.append(loss.item())\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    for data in val_loader:\n","        input = {\n","            'input_values': data['input_values'].to(device),\n","            'attention_mask': data['attention_mask'].to(device)\n","        }\n","        targets = data['target'].to(device)\n","\n","        # calculate loss\n","        outputs = model(input)\n","        loss = loss_func(outputs, targets)\n","        _, predictions = torch.max(outputs, dim=1)\n","        val_acc += torch.sum(predictions == targets)\n","        val_losses.append(loss.item())\n","\n","    train_acc = train_acc.double()/len(train_loader.dataset)\n","    val_acc = val_acc.double()/len(val_loader.dataset)\n","    train_loss = np.mean(train_losses)\n","    val_loss = np.mean(val_losses)\n","\n","    print(f'Train Accuracy: {train_acc}')\n","    print(f'Val Accuracy: {val_acc}')\n","    print(f'Train Loss: {train_loss}')\n","    print(f'Validate Loss: {val_loss}')\n","    return train_loss, val_loss"],"metadata":{"id":"D4OfybuLld87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, val_loader, loss_func, optimizer, epochs, device):\n","    train_losses = []\n","    val_losses = []\n","    for i in range(epochs):\n","        print(f\"Epoch {i+1}\")\n","        train_loss, val_loss = train_single_epoch(model, train_loader, val_loader, loss_func, optimizer, device)\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        print(\"---------------------------\")\n","    print(\"Finished training\")\n","    return train_losses, val_losses"],"metadata":{"id":"1SLU_888MYsh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_loaders(train_df, val_df, test_df, batch_size, label_encoder, device):\n","    train_dataset = Audio_Dataset(\n","        df=train_df,\n","        label_encoder=label_encoder,\n","        device=device)\n","    val_dataset = Audio_Dataset(\n","        df=val_df,\n","        label_encoder=label_encoder,\n","        device=device)\n","    test_dataset = Audio_Dataset(\n","        df=test_df,\n","        label_encoder=label_encoder,\n","        device=device)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","    return train_loader, val_loader, test_loader"],"metadata":{"id":"DV2aGe274kpk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_encoder = {\n","    'khắc hưng': 0,\n","    'châu đăng khoa': 1,\n","    'khắc việt': 2,\n","    'phúc trường': 3,\n","    'nguyễn đình vũ': 4,\n","    'mr siro': 5,\n","    'vương anh tú': 6,\n","    'trịnh công sơn': 7,\n","    'phan mạnh quỳnh': 8,\n","    'nguyên chấn phong': 9,\n","    'nguyễn hồng thuận':10,\n","    'nguyễn văn chung': 11,\n","    'phạm trưởng': 12,\n","    'khánh đơn': 13,\n","    'tiên cookie': 14,\n","}\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"metadata":{"id":"rnbIDXEx7Cec"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 128\n","n_classes = len(label_encoder.keys())\n","\n","train_loader, val_loader, test_loader = prepare_loaders(train_df, val_df, test_df, BATCH_SIZE, label_encoder, device)"],"metadata":{"id":"3-g_v9cP3J2x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Audio_Classifier(n_classes).to(device)"],"metadata":{"id":"Tg38XFC69mEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### freeze n layers of Pretrained Wav2vec2\n","# model.w2v2.feature_extractor._freeze_parameters()\n","\n","# for param in model.w2v2.parameters():\n","#   param.requires_grad = False"],"metadata":{"id":"a8EF5vgzAJ40"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 150\n","LR = 5e-5\n","\n","loss_func = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=LR)\n","\n","train_loss, val_loss = train(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    loss_func=loss_func,\n","    optimizer=optimizer,\n","    epochs=EPOCHS,\n","    device=device\n",")"],"metadata":{"id":"RuUTtJi_lrHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = range(1, EPOCHS + 1)\n","# Plot and label the training and validation loss values\n","plt.figure(figsize=(14,10))\n","plt.plot(epochs, train_loss, label='Training Loss')\n","plt.plot(epochs, val_loss, label='Validation Loss')\n","\n","# Add in a title and axes labels\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","# Set the tick locations\n","plt.xticks(range(0, EPOCHS + 1, 5))\n","\n","# Display the plot\n","plt.legend(loc='best')\n","plt.show()"],"metadata":{"id":"vt6qxGYQENAv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_dnfwtPxGAM6"},"outputs":[],"source":["checkpoint_path = sub_path + 'audio_non_freezed_model_checkpoints.pth'\n","model_path = sub_path + 'audio_non_freezed_model.pth'\n","\n","checkpoint = {\n","    'epoch': EPOCHS + 1,\n","    'state_dict': model.state_dict(),\n","    'optimizer': optimizer.state_dict()\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JnPlQxJLs2Rj"},"outputs":[],"source":["torch.save(checkpoint, checkpoint_path)\n","torch.save(model.state_dict(), model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhIYbzJTHI--"},"outputs":[],"source":["def load_ckp(checkpoint_fpath, model, optimizer, device):\n","    checkpoint = torch.load(checkpoint_fpath, map_location=torch.device(device))\n","    model.load_state_dict(checkpoint['state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    return model, optimizer, checkpoint['epoch']\n","\n","def load_model(model_path, model):\n","    weights = torch.load(model_path, map_location=torch.device(device))\n","    model.load_state_dict(checkpoint)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MWvJqe8a-lGW"},"outputs":[],"source":["# model, optimizer, epoch = load_ckp(checkpoint_path, model, optimizer, device)\n","# model = load_model(model_path, model)"]},{"cell_type":"code","source":["def test(model, data_loader):\n","  model.eval()\n","  predicts = []\n","  predict_probs = []\n","  real_values = []\n","\n","  for data in data_loader:\n","      input = {\n","        'input_values': data['input_values'].to(device),\n","        'attention_mask': data['attention_mask'].to(device)\n","      }\n","      targets = data['target'].to(device)\n","\n","      total_outs = []\n","      with torch.no_grad():\n","        outputs = model(input)\n","        total_outs.append(outputs)\n","\n","      total_outs = torch.stack(total_outs)\n","      _, pred = torch.max(total_outs.mean(0), dim=1)\n","      predicts.extend(pred)\n","      real_values.extend(targets)\n","\n","  predicts = torch.stack(predicts).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return real_values, predicts"],"metadata":{"id":"I7IbTtEimDQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["real_values, pred_values = test(model, test_loader)"],"metadata":{"id":"Qp4JwuPTEJZD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Precision Score: ', round(precision_score(real_values, pred_values, average='macro'),6))\n","print('Recall Score: ', round(recall_score(real_values, pred_values, average='macro'),6))\n","print('F1 Score: ', round(f1_score(real_values, pred_values, average='macro'),6))"],"metadata":{"id":"_cw3ioeqyYJI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_names = label_encoder.keys()\n","print(classification_report(real_values, pred_values, target_names=target_names))"],"metadata":{"id":"_EvSxZxphw09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = label_encoder.keys()\n","\n","\n","disp = ConfusionMatrixDisplay.from_predictions(real_values, pred_values, display_labels=labels, cmap='Oranges')\n","\n","\n","plt.xticks(fontsize=8)\n","\n","plt.gcf().autofmt_xdate()\n","plt.show()\n","\n","fig_name = 'audio_non_freezed_cm.png'\n","disp.figure_.savefig(sub_path + fig_name, format='png', bbox_inches='tight')"],"metadata":{"id":"hMb59Q2s9rV8"},"execution_count":null,"outputs":[]}]}